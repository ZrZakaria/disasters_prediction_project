{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTmdUCNpA6p1"
      },
      "source": [
        "# Régression :\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8aPS44SbTLSm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Colab : installer si nécessaire\n",
        "%pip install xgboost scikit-learn category_encoders --quiet\n",
        "\n",
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold, KFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             confusion_matrix, roc_auc_score, roc_curve, classification_report,\n",
        "                             mean_absolute_error, mean_squared_error, r2_score, silhouette_score)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import category_encoders as ce\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "_ARr37sfTL03",
        "outputId": "70183e91-09b5-418a-8d50-bf838b969ed9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:1: SyntaxWarning: \"\\S\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\S\"? A raw string is also an option.\n",
            "<>:1: SyntaxWarning: \"\\S\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\S\"? A raw string is also an option.\n",
            "C:\\Users\\ZR\\AppData\\Local\\Temp\\ipykernel_2396\\2451327777.py:1: SyntaxWarning: \"\\S\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\S\"? A raw string is also an option.\n",
            "  df = pd.read_csv('..\\STEP1_PRETREATMENT\\DISASTERS_CLEANED.csv')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Year</th>\n",
              "      <th>Disaster Group</th>\n",
              "      <th>Disaster Subgroup</th>\n",
              "      <th>Disaster Type</th>\n",
              "      <th>Disaster Subtype</th>\n",
              "      <th>Country</th>\n",
              "      <th>Region</th>\n",
              "      <th>Continent</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>...</th>\n",
              "      <th>No Affected</th>\n",
              "      <th>No Homeless</th>\n",
              "      <th>Total Affected</th>\n",
              "      <th>Total Damages ('000 US$)</th>\n",
              "      <th>Insured Damages ('000 US$)</th>\n",
              "      <th>Start_Date</th>\n",
              "      <th>End_Date</th>\n",
              "      <th>Duration_Days</th>\n",
              "      <th>Month</th>\n",
              "      <th>Season</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1902</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14.000</td>\n",
              "      <td>-91.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1902-04-18</td>\n",
              "      <td>1902-04-18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1902</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14.400</td>\n",
              "      <td>-90.22</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1902-04-08</td>\n",
              "      <td>1902-04-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1902</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14.400</td>\n",
              "      <td>-90.22</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1902-10-24</td>\n",
              "      <td>1902-10-24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1903</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>50.358</td>\n",
              "      <td>-81.96</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1903-04-29</td>\n",
              "      <td>1903-04-29</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1904</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>22.587</td>\n",
              "      <td>91.13</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1904-11-01</td>\n",
              "      <td>1904-11-01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 23 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Year  Disaster Group  Disaster Subgroup  Disaster Type  Disaster Subtype  \\\n",
              "0  1902               0                  0              0                 0   \n",
              "1  1902               0                  0              1                 1   \n",
              "2  1902               0                  0              1                 1   \n",
              "3  1903               0                  0              2                 2   \n",
              "4  1904               0                  1              3                 3   \n",
              "\n",
              "   Country  Region  Continent  Latitude  Longitude  ...  No Affected  \\\n",
              "0        0       0          0    14.000     -91.00  ...          0.0   \n",
              "1        0       0          0    14.400     -90.22  ...          0.0   \n",
              "2        0       0          0    14.400     -90.22  ...          0.0   \n",
              "3        1       1          0    50.358     -81.96  ...          0.0   \n",
              "4        2       2          1    22.587      91.13  ...          0.0   \n",
              "\n",
              "   No Homeless  Total Affected  Total Damages ('000 US$)  \\\n",
              "0          0.0             0.0                   25000.0   \n",
              "1          0.0             0.0                       0.0   \n",
              "2          0.0             0.0                       0.0   \n",
              "3          0.0            23.0                       0.0   \n",
              "4          0.0             0.0                       0.0   \n",
              "\n",
              "   Insured Damages ('000 US$)  Start_Date    End_Date  Duration_Days Month  \\\n",
              "0                         0.0  1902-04-18  1902-04-18            0.0   4.0   \n",
              "1                         0.0  1902-04-08  1902-04-08            0.0   4.0   \n",
              "2                         0.0  1902-10-24  1902-10-24            0.0  10.0   \n",
              "3                         0.0  1903-04-29  1903-04-29            0.0   4.0   \n",
              "4                         0.0  1904-11-01  1904-11-01            0.0  11.0   \n",
              "\n",
              "  Season  \n",
              "0      0  \n",
              "1      0  \n",
              "2      1  \n",
              "3      0  \n",
              "4      1  \n",
              "\n",
              "[5 rows x 23 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('..\\STEP1_PRETREATMENT\\DISASTERS_CLEANED.csv')\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF6LmrvDU9Bm",
        "outputId": "c604d620-1aaa-43db-8bd9-aac4efa3e425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15818 entries, 0 to 15817\n",
            "Data columns (total 23 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   Year                        15818 non-null  int64  \n",
            " 1   Disaster Group              15818 non-null  int64  \n",
            " 2   Disaster Subgroup           15818 non-null  int64  \n",
            " 3   Disaster Type               15818 non-null  int64  \n",
            " 4   Disaster Subtype            15818 non-null  int64  \n",
            " 5   Country                     15818 non-null  int64  \n",
            " 6   Region                      15818 non-null  int64  \n",
            " 7   Continent                   15818 non-null  int64  \n",
            " 8   Latitude                    15818 non-null  float64\n",
            " 9   Longitude                   15818 non-null  float64\n",
            " 10  Dis Mag Value               15818 non-null  float64\n",
            " 11  Total Deaths                15818 non-null  float64\n",
            " 12  No Injured                  15818 non-null  float64\n",
            " 13  No Affected                 15818 non-null  float64\n",
            " 14  No Homeless                 15818 non-null  float64\n",
            " 15  Total Affected              15818 non-null  float64\n",
            " 16  Total Damages ('000 US$)    15818 non-null  float64\n",
            " 17  Insured Damages ('000 US$)  15818 non-null  float64\n",
            " 18  Start_Date                  15817 non-null  object \n",
            " 19  End_Date                    15817 non-null  object \n",
            " 20  Duration_Days               15818 non-null  float64\n",
            " 21  Month                       15817 non-null  float64\n",
            " 22  Season                      15818 non-null  int64  \n",
            "dtypes: float64(12), int64(9), object(2)\n",
            "memory usage: 2.8+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2KcjhWb8b5K"
      },
      "source": [
        "# prédire Total Deaths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cYlI5ouxTQcH"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"['ISO', 'Location', 'Dis Mag Scale', 'Severity_Score', 'Severity_Category', 'Magnitude_Category'] not found in axis\"",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m      2\u001b[39m cols_to_drop = [\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mISO\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLocation\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMagnitude_Category\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     20\u001b[39m ]\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# 2. On crée X en supprimant ces colonnes (SANS inplace=True)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m X = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcols_to_drop\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\core\\frame.py:5603\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5455\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5456\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5457\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5464\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5465\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5466\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5467\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5468\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5601\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5602\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5605\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5606\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5607\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5608\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5609\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5610\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5611\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\core\\generic.py:4810\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4810\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4813\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\core\\generic.py:4852\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4850\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4851\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4852\u001b[39m         new_axis = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4853\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4855\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4856\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\core\\indexes\\base.py:7136\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7136\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7137\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
            "\u001b[31mKeyError\u001b[39m: \"['ISO', 'Location', 'Dis Mag Scale', 'Severity_Score', 'Severity_Category', 'Magnitude_Category'] not found in axis\""
          ]
        }
      ],
      "source": [
        "# 1. On définit la liste des colonnes à supprimer proprement\n",
        "cols_to_drop = [\n",
        "    \"ISO\",\n",
        "    \"Location\",\n",
        "    \"Dis Mag Scale\",\n",
        "    \"Latitude\",\n",
        "    \"Longitude\",\n",
        "    \"Start_Date\",\n",
        "    \"End_Date\",\n",
        "    \"Month\",\n",
        "    \"No Affected\",\n",
        "    \"No Injured\",\n",
        "    \"No Homeless\",\n",
        "    \"Total Deaths\",\n",
        "    \"Total Affected\",\n",
        "    \"Total Damages ('000 US$)\",\n",
        "    \"Severity_Score\",\n",
        "    \"Severity_Category\",\n",
        "    \"Magnitude_Category\"\n",
        "]\n",
        "\n",
        "# 2. On crée X en supprimant ces colonnes (SANS inplace=True)\n",
        "X = df.drop(columns=cols_to_drop)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHT4cMXk0Bp2",
        "outputId": "08ae8b1a-579b-4e75-ca4f-516841ac41ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15894 entries, 0 to 15893\n",
            "Data columns (total 12 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   Year                        15894 non-null  int64  \n",
            " 1   Disaster Group              15894 non-null  int64  \n",
            " 2   Disaster Subgroup           15894 non-null  int64  \n",
            " 3   Disaster Type               15894 non-null  int64  \n",
            " 4   Disaster Subtype            15894 non-null  int64  \n",
            " 5   Country                     15894 non-null  int64  \n",
            " 6   Region                      15894 non-null  int64  \n",
            " 7   Continent                   15894 non-null  int64  \n",
            " 8   Dis Mag Value               15894 non-null  float64\n",
            " 9   Insured Damages ('000 US$)  15894 non-null  float64\n",
            " 10  Duration_Days               15894 non-null  float64\n",
            " 11  Season                      15894 non-null  int64  \n",
            "dtypes: float64(3), int64(9)\n",
            "memory usage: 1.5 MB\n"
          ]
        }
      ],
      "source": [
        "X.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2zDvUOaTQh3"
      },
      "outputs": [],
      "source": [
        "# Normalisation\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzB1UTdVxbew"
      },
      "outputs": [],
      "source": [
        "# Préparer X/y pour régression (ex : prédire Total Deaths)\n",
        "target_deaths = 'Total Deaths'  # adapte\n",
        "assert target_deaths in df.columns, f\"{target_deaths} absent\"\n",
        "\n",
        "# Exclure target features leak (ne pas inclure SeverityScore/SeverityLevel)\n",
        "X_reg = X.copy()\n",
        "y_reg = df[target_deaths].fillna(df[target_deaths].median()).values\n",
        "\n",
        "# 1. Gestion des valeurs extrêmes (Capping)\n",
        "# On fixe le maximum au 99ème percentile (environ 3500 morts)\n",
        "limit = df['Total Deaths'].quantile(0.99)\n",
        "y_capped = np.clip(df['Total Deaths'], 0, limit)\n",
        "\n",
        "# 2. Transformation Logarithmique\n",
        "# On applique log(1+x) pour écraser l'échelle\n",
        "y_final = np.log1p(y_capped)\n",
        "\n",
        "# Ensuite, entraînez vos modèles sur 'y_final'\n",
        "\n",
        "# split\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_scaled, y_final, test_size=0.2, random_state=42)\n",
        "\n",
        "# Eval\n",
        "def eval_reg(y_true, y_pred):\n",
        "    print(\"MAE:\", mean_absolute_error(y_true, y_pred))\n",
        "    print(\"RMSE:\", np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "    print(\"R2:\", r2_score(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgzSSKKNyf3z",
        "outputId": "9ad023e6-3dcd-43ac-89d3-b0388200d93b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForestRegressor\n",
            "MAE: 1.2089912557525266\n",
            "RMSE: 1.5701749306105044\n",
            "R2: 0.3984106902272274\n",
            "\n",
            "\n",
            "XGBoost\n",
            "MAE: 1.1789444933461537\n",
            "RMSE: 1.5473003165252286\n",
            "R2: 0.4158111539219165\n",
            "\n",
            "\n",
            "Linear Regression\n",
            "MAE: 1.4282145883223016\n",
            "RMSE: 1.8055086783171534\n",
            "R2: 0.2045677547496404\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# RandomForestRegressor\n",
        "rfr_deaths = RandomForestRegressor(n_estimators=200, max_depth=12, random_state=42)\n",
        "rfr_deaths.fit(X_train_r, y_train_r)\n",
        "pred = rfr_deaths.predict(X_test_r)\n",
        "\n",
        "# XGBoost\n",
        "xgbr_deaths = xgb.XGBRegressor(n_estimators=200, random_state=42, objective='reg:squarederror')\n",
        "xgbr_deaths.fit(X_train_r, y_train_r)\n",
        "\n",
        "# Linear Regression baseline\n",
        "lr_deaths = LinearRegression()\n",
        "lr_deaths.fit(X_train_r, y_train_r)\n",
        "\n",
        "# Evaluation des models\n",
        "print(\"RandomForestRegressor\")\n",
        "eval_reg(y_test_r, rfr_deaths.predict(X_test_r))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"XGBoost\")\n",
        "eval_reg(y_test_r, xgbr_deaths.predict(X_test_r))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Linear Regression\")\n",
        "eval_reg(y_test_r, lr_deaths.predict(X_test_r))\n",
        "print(\"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M_CuEcP76DN"
      },
      "source": [
        "# Choix du meilleur algorithme de régréssion pour la prédiction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2AFbNi17qT1"
      },
      "source": [
        "1. Le Grand Gagnant : XGBoost (et Random Forest)Avant : $R^2 \\approx 0.12$ (Le modèle ne comprenait rien).Maintenant : $R^2 \\approx 0.53$ (Le modèle capture plus de 50% de la logique des catastrophes).Verdict : Oui, ces modèles sont maintenant \"bien entraînés\".Un score $R^2$ de 0.53 est très respectable pour des données aussi complexes que des catastrophes naturelles (où le hasard joue un grand rôle). Vous ne monterez probablement pas beaucoup plus haut sans données externes supplémentaires (densité de population, qualité des infrastructures, etc.).\n",
        "2. Comment interpréter cette \"MAE de 1.05\" ?C'est le point le plus important pour défendre votre travail.Comme vous avez utilisé une transformation logarithmique (log(y+1)), une MAE de 1.05 ne signifie pas \"1 mort d'erreur\".Cela signifie que votre modèle est précis à un \"facteur\" près.Mathématiquement : $e^{1.05} \\approx 2.85$.Interprétation métier : En moyenne, la prédiction de votre modèle est dans un facteur de 3 par rapport à la réalité.S'il y a eu 30 morts, le modèle prédit entre 10 et 90.S'il y a eu 3 000 morts, le modèle prédit entre 1 000 et 9 000.Pour la gestion de catastrophes (où l'on veut savoir si c'est un événement mineur ou une crise humanitaire), cette précision est utile et exploitable.\n",
        "3. Le Cas \"Régression Linéaire\"$R^2 = 0.21$. C'est mieux qu'avant (0.007), mais cela confirme que la relation entre vos variables et le nombre de morts n'est pas linéaire. C'est une bonne preuve à inclure dans votre rapport pour justifier l'usage de modèles complexes comme XGBoost.\n",
        "\n",
        "Conclusion pour votre évaluationVous pouvez maintenant affirmer avec confiance :Que vous avez identifié un problème de distribution (événements extrêmes).Que vous l'avez corrigé (Log-transformation + Capping).Que vous avez obtenu un modèle XGBoost performant ($R^2=0.53$) capable d'estimer l'ampleur d'une catastrophe avec une marge d'erreur raisonnable pour ce domaine.C'est un travail de qualité Data Science validé."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7FK8F5m7gjT"
      },
      "source": [
        "# Sauvegarde du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvh91hIT7fm_",
        "outputId": "8e1c2063-4ece-4280-a090-2c3ff5c4e7c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Modèle sauvegardé sous le nom : totaldeaths_model_xgboost.pkl\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "filename = 'totaldeaths_model_xgboost.pkl'\n",
        "with open(filename, 'wb') as file:\n",
        "    pickle.dump(xgbr_deaths, file)\n",
        "\n",
        "print(f\" Modèle sauvegardé sous le nom : {filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZN7dxYaYlB0"
      },
      "source": [
        "# prédire Total Affected\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbe3Wb_YdgvC"
      },
      "outputs": [],
      "source": [
        "# 1. On définit la liste des colonnes à supprimer proprement\n",
        "cols_to_drop = [\n",
        "    \"ISO\",\n",
        "    \"Location\",\n",
        "    \"Dis Mag Scale\",\n",
        "    \"Latitude\",\n",
        "    \"Longitude\",\n",
        "    \"Start_Date\",\n",
        "    \"End_Date\",\n",
        "    \"Month\",\n",
        "    \"No Affected\",\n",
        "    \"No Injured\",\n",
        "    \"No Homeless\",\n",
        "    \"Total Deaths\",\n",
        "    \"Total Affected\",\n",
        "    \"Total Damages ('000 US$)\",\n",
        "    \"Severity_Score\",\n",
        "    \"Severity_Category\",\n",
        "    \"Magnitude_Category\"\n",
        "]\n",
        "\n",
        "# 2. On crée X en supprimant ces colonnes (SANS inplace=True)\n",
        "X = df.drop(columns=cols_to_drop)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-8EI_rCdg0K"
      },
      "outputs": [],
      "source": [
        "# Normalisation\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIpt7SSI9O3P"
      },
      "outputs": [],
      "source": [
        "# Préparer X/y pour régression (ex : prédire Total Deaths)\n",
        "target_affected = 'Total Affected'  # adapte\n",
        "assert target_affected in df.columns, f\"{target_affected} absent\"\n",
        "\n",
        "# Exclure target features leak (ne pas inclure SeverityScore/SeverityLevel)\n",
        "X_reg = X.copy()\n",
        "y_reg = df[target_affected].fillna(df[target_affected].median()).values\n",
        "\n",
        "# 1. Gestion des valeurs extrêmes (Capping)\n",
        "# On fixe le maximum au 99ème percentile (environ 3500 morts)\n",
        "limit = df['Total Affected'].quantile(0.99)\n",
        "y_capped = np.clip(df['Total Affected'], 0, limit)\n",
        "\n",
        "# 2. Transformation Logarithmique\n",
        "# On applique log(1+x) pour écraser l'échelle\n",
        "y_final = np.log1p(y_capped)\n",
        "\n",
        "# Ensuite, entraînez vos modèles sur 'y_final'\n",
        "\n",
        "# split\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_scaled, y_final, test_size=0.2, random_state=42)\n",
        "\n",
        "# Eval\n",
        "def eval_reg(y_true, y_pred):\n",
        "    print(\"MAE:\", mean_absolute_error(y_true, y_pred))\n",
        "    print(\"RMSE:\", np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "    print(\"R2:\", r2_score(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0Ar8s0TaSNU",
        "outputId": "664505ba-5bd8-4f3e-8096-3d30311194ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForestRegressor\n",
            "MAE: 2.9308347454599537\n",
            "RMSE: 3.789087355163683\n",
            "R2: 0.3553445509921175\n",
            "\n",
            "\n",
            "XGBoost\n",
            "MAE: 2.8914639163104767\n",
            "RMSE: 3.8029555684289136\n",
            "R2: 0.35061698512082784\n",
            "\n",
            "\n",
            "Linear Regression\n",
            "MAE: 3.6719436867796307\n",
            "RMSE: 4.390113903235162\n",
            "R2: 0.13461373300374668\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# RandomForestRegressor\n",
        "rfr_affected = RandomForestRegressor(n_estimators=200, max_depth=12, random_state=42)\n",
        "rfr_affected.fit(X_train_r, y_train_r)\n",
        "pred = rfr_affected.predict(X_test_r)\n",
        "\n",
        "# XGBoost\n",
        "xgbr_affected = xgb.XGBRegressor(n_estimators=200, random_state=42, objective='reg:squarederror')\n",
        "xgbr_affected.fit(X_train_r, y_train_r)\n",
        "\n",
        "# Linear Regression baseline\n",
        "lr_affected = LinearRegression()\n",
        "lr_affected.fit(X_train_r, y_train_r)\n",
        "\n",
        "# Evaluation des models\n",
        "print(\"RandomForestRegressor\")\n",
        "eval_reg(y_test_r, rfr_affected.predict(X_test_r))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"XGBoost\")\n",
        "eval_reg(y_test_r, xgbr_affected.predict(X_test_r))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Linear Regression\")\n",
        "eval_reg(y_test_r, lr_affected.predict(X_test_r))\n",
        "print(\"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGcIP_nwyswi"
      },
      "source": [
        "Personnes affectées (Total Affected)Meilleur modèle : Random ForestScore ($R^2$) : 0,355Analyse : Le score a chuté par rapport à vos essais précédents (où il était de 0,99), ce qui est une excellente nouvelle. Cela prouve que vous ne \"trichez\" plus en utilisant le bilan pour prédire le bilan. Un score de 0,35 est honnête pour prédire l'impact humain global uniquement avec des données contextuelles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu89AvAAvnmt",
        "outputId": "7873548f-8dfc-41d9-b310-efd4916c9283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Modèle sauvegardé sous le nom : totalaffected_model_randomforest.pkl\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "filename = 'totalaffected_model_randomforest.pkl'\n",
        "with open(filename, 'wb') as file:\n",
        "    pickle.dump(rfr_affected, file)\n",
        "\n",
        "print(f\" Modèle sauvegardé sous le nom : {filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zgbXprlfdO-"
      },
      "source": [
        "# prédire Total Damages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viAzgPrsflue"
      },
      "outputs": [],
      "source": [
        "# 1. On définit la liste des colonnes à supprimer proprement\n",
        "cols_to_drop = [\n",
        "    \"ISO\",\n",
        "    \"Location\",\n",
        "    \"Dis Mag Scale\",\n",
        "    \"Latitude\",\n",
        "    \"Longitude\",\n",
        "    \"Start_Date\",\n",
        "    \"End_Date\",\n",
        "    \"Month\",\n",
        "    \"No Affected\",\n",
        "    \"No Injured\",\n",
        "    \"No Homeless\",\n",
        "    \"Total Deaths\",\n",
        "    \"Total Affected\",\n",
        "    \"Total Damages ('000 US$)\",\n",
        "    \"Severity_Score\",\n",
        "    \"Severity_Category\",\n",
        "    \"Magnitude_Category\"\n",
        "]\n",
        "\n",
        "# 2. On crée X en supprimant ces colonnes (SANS inplace=True)\n",
        "X = df.drop(columns=cols_to_drop)\n",
        "# Normalisation\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yZLMh8uflzX"
      },
      "outputs": [],
      "source": [
        "# Préparer X/y pour régression (ex : prédire Total Deaths)\n",
        "target_damages = \"Total Damages ('000 US$)\"  # adapte\n",
        "assert target_damages in df.columns, f\"{target_damages} absent\"\n",
        "\n",
        "# Exclure target features leak (ne pas inclure SeverityScore/SeverityLevel)\n",
        "X_reg = X.copy()\n",
        "y_reg = df[target_damages].fillna(df[target_damages].median()).values\n",
        "\n",
        "# 1. Gestion des valeurs extrêmes (Capping)\n",
        "# On fixe le maximum au 99ème percentile (environ 3500 morts)\n",
        "limit = df[\"Total Damages ('000 US$)\"].quantile(0.99)\n",
        "y_capped = np.clip(df[\"Total Damages ('000 US$)\"], 0, limit)\n",
        "\n",
        "# 2. Transformation Logarithmique\n",
        "# On applique log(1+x) pour écraser l'échelle\n",
        "y_final = np.log1p(y_capped)\n",
        "\n",
        "# Ensuite, entraînez vos modèles sur 'y_final'\n",
        "\n",
        "# split\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_scaled, y_final, test_size=0.2, random_state=42)\n",
        "\n",
        "# Eval\n",
        "def eval_reg(y_true, y_pred):\n",
        "    print(\"MAE:\", mean_absolute_error(y_true, y_pred))\n",
        "    print(\"RMSE:\", np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "    print(\"R2:\", r2_score(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LOgEE7Ufl8i",
        "outputId": "00029098-2548-48d0-d3fe-316c886c3171"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForestRegressor\n",
            "MAE: 3.055116404566466\n",
            "RMSE: 4.08494246505009\n",
            "R2: 0.40066265608270113\n",
            "\n",
            "\n",
            "XGBoost\n",
            "MAE: 2.9769107960615697\n",
            "RMSE: 4.08811680575658\n",
            "R2: 0.39973082404595595\n",
            "\n",
            "\n",
            "Linear Regression\n",
            "MAE: 4.364216064937665\n",
            "RMSE: 5.016856537676907\n",
            "R2: 0.09601168877436994\n"
          ]
        }
      ],
      "source": [
        "# RandomForestRegressor\n",
        "rfr_damages = RandomForestRegressor(n_estimators=200, max_depth=12, random_state=42)\n",
        "rfr_damages.fit(X_train_r, y_train_r)\n",
        "pred = rfr_damages.predict(X_test_r)\n",
        "\n",
        "# XGBoost\n",
        "xgbr_damages = xgb.XGBRegressor(n_estimators=200, random_state=42, objective='reg:squarederror')\n",
        "xgbr_damages.fit(X_train_r, y_train_r)\n",
        "\n",
        "# Linear Regression baseline\n",
        "lr_damages = LinearRegression()\n",
        "lr_damages.fit(X_train_r, y_train_r)\n",
        "\n",
        "# Evaluation des models\n",
        "print(\"RandomForestRegressor\")\n",
        "eval_reg(y_test_r, rfr_damages.predict(X_test_r))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"XGBoost\")\n",
        "eval_reg(y_test_r, xgbr_damages.predict(X_test_r))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Linear Regression\")\n",
        "eval_reg(y_test_r, lr_damages.predict(X_test_r))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPw7M28VzIHg"
      },
      "source": [
        " Dommages financiers (Total Damages)Meilleur modèle : Random ForestScore ($R^2$) : 0,401Analyse : La forêt aléatoire et XGBoost sont quasiment à égalité (0,401 vs 0,400). La forêt aléatoire est légèrement plus stable ici."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWQICTc9fmN3",
        "outputId": "f2df5314-e914-4649-c269-ada2838e79e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Modèle sauvegardé sous le nom : totaldamages_model_randomforest.pkl\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "filename = 'totaldamages_model_randomforest.pkl'\n",
        "with open(filename, 'wb') as file:\n",
        "    pickle.dump(rfr_damages, file)\n",
        "\n",
        "print(f\" Modèle sauvegardé sous le nom : {filename}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
